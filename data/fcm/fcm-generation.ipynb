{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c17a308",
   "metadata": {},
   "source": [
    "# __Cargar datos__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1480fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: NYU, Total: 177, TDAH: 90, Control: 87\n",
      "**********\n",
      "Dataset: NeuroIMAGE, Total: 39, TDAH: 17, Control: 22\n",
      "**********\n",
      "Dataset: KKI, Total: 78, TDAH: 20, Control: 58\n",
      "**********\n",
      "Dataset: OHSU, Total: 66, TDAH: 28, Control: 38\n",
      "**********\n",
      "Dataset: Peking, Total: 183, TDAH: 74, Control: 109\n",
      "**********\n",
      "\n",
      "Total de muestras: 543\n",
      "Total con TDHA: 229\n",
      "NÃºmero de sitios: 5\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "dir_list = [\n",
    "    {\n",
    "    \"dir_name\": \"NYU\",\n",
    "    \"single_phenotype_file\": True,\n",
    "    },\n",
    "    {\n",
    "    \"dir_name\": \"NeuroIMAGE\",\n",
    "    \"single_phenotype_file\": True,\n",
    "    },\n",
    "    {\n",
    "    \"dir_name\": \"KKI\",\n",
    "    \"single_phenotype_file\": True,\n",
    "    },\n",
    "    {\n",
    "    \"dir_name\": \"OHSU\",\n",
    "    \"single_phenotype_file\": True,\n",
    "    },\n",
    "    {\n",
    "    \"dir_name\": \"Peking\",\n",
    "    \"single_phenotype_file\": False,\n",
    "    },\n",
    "    ]\n",
    "\n",
    "\n",
    "n_rois=18\n",
    "\n",
    "all_data = {}\n",
    "site2idx = {}         # Diccionario para codificar los sitios como enteros\n",
    "train_ts_list = []\n",
    "train_labels = []\n",
    "train_sites = []      # <--- Lista paralela con los sitios\n",
    "\n",
    "for idx, dataset in enumerate(dir_list):\n",
    "    dir_name = dataset[\"dir_name\"]\n",
    "    site2idx[dir_name] = idx          # Asignamos un entero a cada sitio\n",
    "\n",
    "    data = joblib.load(f\"../raw-bold-data/{n_rois}-rois-dataset/{dir_name}.pkl\")\n",
    "    ts_list, labels = data[\"data\"], data[\"labels\"]\n",
    "    all_data[dir_name] = {\"data\": ts_list, \"labels\": labels}\n",
    "\n",
    "    train_ts_list.extend(ts_list)\n",
    "    train_labels.extend(labels)\n",
    "    train_sites.extend([idx] * len(ts_list))  # AÃ±adir cÃ³digo del sitio para cada muestra\n",
    "\n",
    "    print(\"Dataset: {}, Total: {}, TDAH: {}, Control: {}\".format(dir_name, len(ts_list), sum(labels),\n",
    "                                                               len(ts_list) - sum(labels)))\n",
    "    print(10 * \"*\")\n",
    "\n",
    "idx2site = {v: k for k, v in site2idx.items()}\n",
    "\n",
    "# EstadÃ­sticas\n",
    "print(\"\\nTotal de muestras:\", len(train_ts_list))\n",
    "print(\"Total con TDHA:\", sum(train_labels))\n",
    "print(\"NÃºmero de sitios:\", len(site2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a527724",
   "metadata": {},
   "source": [
    "## __FC GENERATOR__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3779422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DynamicFcBuilder:\n",
    "    def __init__(self, win_len=40, step=15):\n",
    "        self.win_len = win_len\n",
    "        self.step = step\n",
    "\n",
    "    def sliding_windows(self, ts):\n",
    "        ts = np.ascontiguousarray(ts)  # mejora acceso a memoria\n",
    "        T = ts.shape[1]\n",
    "        starts = range(0, T - self.win_len + 1, self.step)\n",
    "        return np.stack([ts[:, s:s+self.win_len] for s in starts], axis=0)\n",
    "\n",
    "    def ts_to_fc(self, windows):\n",
    "        fcs = []\n",
    "        for w in windows:\n",
    "            # NormalizaciÃ³n robusta\n",
    "            mu = w.mean(axis=1, keepdims=True)\n",
    "            std = w.std(axis=1, keepdims=True)\n",
    "            std[std == 0.0] = 1.0\n",
    "            w_norm = (w - mu) / std\n",
    "\n",
    "            # CorrelaciÃ³n rÃ¡pida con z-transformaciÃ³n\n",
    "            r = np.dot(w_norm, w_norm.T) / w.shape[1]\n",
    "            np.fill_diagonal(r, 0.0)\n",
    "            z = np.arctanh(np.clip(r, -0.999, 0.999))\n",
    "            fcs.append(z.astype(np.float32))\n",
    "\n",
    "        return np.stack(fcs)\n",
    "\n",
    "    def compute_dynamic_fc(self, ts):\n",
    "        return self.ts_to_fc(self.sliding_windows(ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca0e6de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sitio  Sujetos  Matrices FC por sujeto  Matrices FC por sitio\n",
      "0         NYU      177                       9                   1593\n",
      "1  NeuroIMAGE       39                      15                    585\n",
      "2         KKI       78                       6                    522\n",
      "3        OHSU       66                       3                    198\n",
      "4      Peking      183                      13                   2379\n"
     ]
    }
   ],
   "source": [
    "fc_builder = DynamicFcBuilder(win_len=40, step=15)\n",
    "\n",
    "resumen_data = []\n",
    "\n",
    "for site_name, site_data in all_data.items():\n",
    "    ts_list = site_data[\"data\"]\n",
    "    matrices_por_sujeto = [fc_builder.compute_dynamic_fc(ts).shape[0] for ts in ts_list]\n",
    "    total_fc = sum(matrices_por_sujeto)\n",
    "    promedio_fc = np.mean(matrices_por_sujeto)\n",
    "\n",
    "    resumen_data.append({\n",
    "        \"Sitio\": site_name,\n",
    "        \"Sujetos\": len(ts_list),\n",
    "        \"Matrices FC por sujeto\": int(promedio_fc),\n",
    "        \"Matrices FC por sitio\": total_fc,\n",
    "\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "df_resumen = pd.DataFrame(resumen_data)\n",
    "print(df_resumen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da6d75",
   "metadata": {},
   "source": [
    "#  __FCM-RAW-BOLD-GENERATOR__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "056eb3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NYU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 177/177 [00:00<00:00, 3071.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NYU] sujetos: 177 | guardados: 177 | saltados: 0\n",
      "â†’ 18-fcm/NYU_dfc.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NeuroIMAGE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 2390.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeuroIMAGE] sujetos: 39 | guardados: 39 | saltados: 0\n",
      "â†’ 18-fcm/NeuroIMAGE_dfc.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KKI: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:00<00:00, 5166.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KKI] sujetos: 78 | guardados: 78 | saltados: 0\n",
      "â†’ 18-fcm/KKI_dfc.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OHSU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66/66 [00:00<00:00, 4532.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OHSU] sujetos: 66 | guardados: 66 | saltados: 0\n",
      "â†’ 18-fcm/OHSU_dfc.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Peking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 183/183 [00:00<00:00, 3070.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Peking] sujetos: 183 | guardados: 183 | saltados: 0\n",
      "â†’ 18-fcm/Peking_dfc.pkl\n",
      "\n",
      "Resumen por sitio:\n",
      "        Sitio  Sujetos  Matrices FC por sujeto  Matrices FC por sitio\n",
      "0         NYU      177                       9                   1593\n",
      "1  NeuroIMAGE       39                      15                    585\n",
      "2         KKI       78                       7                    522\n",
      "3        OHSU       66                       3                    198\n",
      "4      Peking      183                      13                   2379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Tu clase, tal cual ===\n",
    "import numpy as np\n",
    "\n",
    "class DynamicFcBuilder:\n",
    "    def __init__(self, win_len=40, step=15):\n",
    "        self.win_len = win_len\n",
    "        self.step = step\n",
    "\n",
    "    def sliding_windows(self, ts):\n",
    "        ts = np.ascontiguousarray(ts)  # mejora acceso a memoria\n",
    "        T = ts.shape[1]\n",
    "        starts = range(0, T - self.win_len + 1, self.step)\n",
    "        return np.stack([ts[:, s:s+self.win_len] for s in starts], axis=0)\n",
    "\n",
    "    def ts_to_fc(self, windows):\n",
    "        fcs = []\n",
    "        for w in windows:\n",
    "            # NormalizaciÃ³n robusta\n",
    "            mu = w.mean(axis=1, keepdims=True)\n",
    "            std = w.std(axis=1, keepdims=True)\n",
    "            std[std == 0.0] = 1.0\n",
    "            w_norm = (w - mu) / std\n",
    "\n",
    "            # CorrelaciÃ³n rÃ¡pida con z-transformaciÃ³n\n",
    "            r = np.dot(w_norm, w_norm.T) / w.shape[1]\n",
    "            np.fill_diagonal(r, 0.0)\n",
    "            z = np.arctanh(np.clip(r, -0.999, 0.999))\n",
    "            fcs.append(z.astype(np.float32))\n",
    "\n",
    "        return np.stack(fcs)\n",
    "\n",
    "    def compute_dynamic_fc(self, ts):\n",
    "        return self.ts_to_fc(self.sliding_windows(ts))\n",
    "\n",
    "\n",
    "# ====== ParÃ¡metros ======\n",
    "n_rois   = 18\n",
    "win_len  = 40\n",
    "step     = 15\n",
    "fc_builder = DynamicFcBuilder(win_len=win_len, step=step)\n",
    "\n",
    "# Directorio de salida\n",
    "OUT_DIR = \"18-fcm\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# all_data debe existir (como lo creaste antes):\n",
    "# all_data = {\"NYU\": {\"data\": [...], \"labels\": [...]}, ...}\n",
    "\n",
    "resumen_data = []\n",
    "\n",
    "def process_site(site_name: str, site_data: dict):\n",
    "    \"\"\"\n",
    "    Genera dFC por sujeto (W_i, 116, 116) y guarda {site_name}_dfc.pkl.\n",
    "    Estructura de guardado:\n",
    "      {\n",
    "        \"site\": str,\n",
    "        \"win_len\": int,\n",
    "        \"step\": int,\n",
    "        \"n_rois\": int,\n",
    "        \"data\":   [np.ndarray (W_i, 116, 116), ...],\n",
    "        \"labels\": [int, ...]\n",
    "      }\n",
    "    Retorna: (num_sujetos, matrices_por_sujeto_list)\n",
    "    \"\"\"\n",
    "    ts_list   = site_data[\"data\"]\n",
    "    labels    = site_data[\"labels\"]\n",
    "\n",
    "    out_obj = {\n",
    "        \"site\": site_name,\n",
    "        \"win_len\": win_len,\n",
    "        \"step\": step,\n",
    "        \"n_rois\": n_rois,\n",
    "        \"data\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "\n",
    "    matrices_por_sujeto = []\n",
    "    skipped = 0\n",
    "\n",
    "    for ts, y in tqdm(zip(ts_list, labels), total=len(ts_list), desc=f\"{site_name}\"):\n",
    "        # Validaciones rÃ¡pidas\n",
    "        if ts.shape[0] != n_rois:\n",
    "            raise ValueError(f\"{site_name}: ROI mismatch. Esperado {n_rois}, recibido {ts.shape[0]}\")\n",
    "        T = ts.shape[1]\n",
    "        if T < win_len:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # dFC\n",
    "        dfc = fc_builder.compute_dynamic_fc(ts)  # (W_i, 116, 116)\n",
    "        if not np.isfinite(dfc).all():\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        out_obj[\"data\"].append(dfc)\n",
    "        out_obj[\"labels\"].append(int(y))\n",
    "        matrices_por_sujeto.append(dfc.shape[0])\n",
    "\n",
    "    # Guardar por sitio\n",
    "    out_path = os.path.join(OUT_DIR, f\"{site_name}_dfc.pkl\")\n",
    "    joblib.dump(out_obj, out_path, compress=3)\n",
    "\n",
    "    print(f\"[{site_name}] sujetos: {len(ts_list)} | guardados: {len(out_obj['data'])} | saltados: {skipped}\")\n",
    "    print(f\"â†’ {out_path}\")\n",
    "\n",
    "    return len(out_obj[\"data\"]), matrices_por_sujeto\n",
    "\n",
    "\n",
    "# ====== Ejecutar por cada sitio y armar resumen ======\n",
    "for site_name, site_data in all_data.items():\n",
    "    sujetos_guardados, mats_ps = process_site(site_name, site_data)\n",
    "\n",
    "    if len(mats_ps) == 0:\n",
    "        promedio_fc = 0\n",
    "        total_fc = 0\n",
    "    else:\n",
    "        promedio_fc = int(np.round(np.mean(mats_ps)))\n",
    "        total_fc = int(np.sum(mats_ps))\n",
    "\n",
    "    resumen_data.append({\n",
    "        \"Sitio\": site_name,\n",
    "        \"Sujetos\": sujetos_guardados,\n",
    "        \"Matrices FC por sujeto\": promedio_fc,\n",
    "        \"Matrices FC por sitio\": total_fc,\n",
    "    })\n",
    "\n",
    "df_resumen = pd.DataFrame(resumen_data)\n",
    "print(\"\\nResumen por sitio:\")\n",
    "print(df_resumen)\n",
    "\n",
    "# (Opcional) guardar el resumen a CSV\n",
    "df_resumen.to_csv(os.path.join(OUT_DIR, \"resumen_dfc_por_sitio.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37958503",
   "metadata": {},
   "source": [
    "# __FCM-AUGMENTED-BOLD__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afe330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========= Clases (tal cual las pasaste) =========\n",
    "class BoldAugmentor:\n",
    "    def __init__(self, noise_std=0.02, shift_max=5,\n",
    "                 flip_prob=0.1, drop_prob=0.1, scale_range=(0.95, 1.05)):\n",
    "        self.noise_std = noise_std\n",
    "        self.shift_max = shift_max\n",
    "        self.flip_prob = flip_prob\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_range = scale_range\n",
    "\n",
    "    def augment(self, ts):\n",
    "        ts = np.ascontiguousarray(ts)\n",
    "\n",
    "        rand = np.random.rand\n",
    "        shape = ts.shape\n",
    "\n",
    "        if rand() < 0.5:\n",
    "            ts += np.random.normal(0.0, self.noise_std, size=shape)\n",
    "\n",
    "        if rand() < 0.3:\n",
    "            shift = np.random.randint(-self.shift_max, self.shift_max + 1)\n",
    "            if shift != 0:\n",
    "                if shift > 0:\n",
    "                    ts = np.pad(ts, ((0, 0), (shift, 0)), mode='constant')[:, :-shift]\n",
    "                else:\n",
    "                    ts = np.pad(ts, ((0, 0), (0, -shift)), mode='constant')[:, -shift:]\n",
    "\n",
    "        if rand() < self.flip_prob:\n",
    "            ts = ts[:, ::-1].copy()\n",
    "\n",
    "        if rand() < self.drop_prob:\n",
    "            drop_idx = np.random.randint(shape[0])\n",
    "            ts[drop_idx] = 0.0\n",
    "\n",
    "        if rand() < self.drop_prob:\n",
    "            ts *= np.random.uniform(*self.scale_range, size=(shape[0], 1))\n",
    "\n",
    "        return np.nan_to_num(ts, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "\n",
    "class DynamicFcBuilder:\n",
    "    def __init__(self, win_len=40, step=15):\n",
    "        self.win_len = win_len\n",
    "        self.step = step\n",
    "\n",
    "    def sliding_windows(self, ts):\n",
    "        ts = np.ascontiguousarray(ts)\n",
    "        T = ts.shape[1]\n",
    "        starts = range(0, T - self.win_len + 1, self.step)\n",
    "        return np.stack([ts[:, s:s+self.win_len] for s in starts], axis=0)\n",
    "\n",
    "    def ts_to_fc(self, windows):\n",
    "        fcs = []\n",
    "        for w in windows:\n",
    "            mu = w.mean(axis=1, keepdims=True)\n",
    "            std = w.std(axis=1, keepdims=True)\n",
    "            std[std == 0.0] = 1.0\n",
    "            w_norm = (w - mu) / std\n",
    "\n",
    "            r = (w_norm @ w_norm.T) / w.shape[1]\n",
    "            np.fill_diagonal(r, 0.0)\n",
    "            z = np.arctanh(np.clip(r, -0.999, 0.999))\n",
    "            fcs.append(z.astype(np.float32))\n",
    "        return np.stack(fcs)\n",
    "\n",
    "    def compute_dynamic_fc(self, ts):\n",
    "        return self.ts_to_fc(self.sliding_windows(ts))\n",
    "\n",
    "\n",
    "# ========= ParÃ¡metros =========\n",
    "n_rois        = 116\n",
    "win_len       = 40\n",
    "step          = 15\n",
    "AUG_REPS      = 2         # cuÃ¡ntas rÃ©plicas aumentadas por sujeto\n",
    "INCLUDE_ORIG  = True      # incluir tambiÃ©n la serie original\n",
    "RANDOM_SEED   = 42        # para reproducibilidad\n",
    "\n",
    "augmentor = BoldAugmentor(\n",
    "    noise_std=0.02,\n",
    "    shift_max=5,\n",
    "    flip_prob=0.1,\n",
    "    drop_prob=0.1,\n",
    "    scale_range=(0.95, 1.05),\n",
    ")\n",
    "fc_builder = DynamicFcBuilder(win_len=win_len, step=step)\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "OUT_DIR = \"../processed-dfc-aug\"   # salida\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# all_data debe existir: {\"NYU\":{\"data\":[(116,T),...],\"labels\":[...]}, ...}\n",
    "\n",
    "def windows_count(T, win_len, step):\n",
    "    return 0 if T < win_len else 1 + (T - win_len) // step\n",
    "\n",
    "\n",
    "def process_site_with_augmentation(site_name: str, site_data: dict):\n",
    "    \"\"\"\n",
    "    Genera (original + AUG_REPS aumentadas) por sujeto, calcula dFC y guarda {site}_dfc_aug.pkl\n",
    "    Estructura:\n",
    "      {\n",
    "        \"site\": str,\n",
    "        \"win_len\": int, \"step\": int, \"n_rois\": int,\n",
    "        \"include_original\": bool, \"aug_reps\": int, \"random_seed\": int,\n",
    "        \"data\":   [ (W_i, 116, 116), ... ],\n",
    "        \"labels\": [ int, ... ],\n",
    "        \"augmented\": [ bool, ... ],        # True si es rÃ©plica aumentada\n",
    "        \"source_index\": [ int, ... ]       # Ã­ndice del sujeto original\n",
    "      }\n",
    "    \"\"\"\n",
    "    ts_list = site_data[\"data\"]\n",
    "    labels  = site_data[\"labels\"]\n",
    "\n",
    "    out_obj = {\n",
    "        \"site\": site_name,\n",
    "        \"win_len\": win_len,\n",
    "        \"step\": step,\n",
    "        \"n_rois\": n_rois,\n",
    "        \"include_original\": INCLUDE_ORIG,\n",
    "        \"aug_reps\": AUG_REPS,\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"data\": [],\n",
    "        \"labels\": [],\n",
    "        \"augmented\": [],\n",
    "        \"source_index\": [],\n",
    "    }\n",
    "\n",
    "    mats_por_sample = []  # W_i por cada muestra (orig o aug)\n",
    "    skipped = 0\n",
    "\n",
    "    for subj_idx, (ts, y) in enumerate(tqdm(zip(ts_list, labels), total=len(ts_list), desc=site_name)):\n",
    "        # validaciones base\n",
    "        if ts.shape[0] != n_rois:\n",
    "            raise ValueError(f\"{site_name}: ROI mismatch. Esperado {n_rois}, recibido {ts.shape[0]}\")\n",
    "        T = ts.shape[1]\n",
    "        if T < win_len:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # ColecciÃ³n de series a convertir a dFC: original (opcional) + rÃ©plicas aumentadas\n",
    "        series_list = []\n",
    "        if INCLUDE_ORIG:\n",
    "            series_list.append((ts, False))  # (serie, es_aug)\n",
    "\n",
    "        for _ in range(AUG_REPS):\n",
    "            ts_aug = augmentor.augment(ts.copy())\n",
    "            series_list.append((ts_aug, True))\n",
    "\n",
    "        # Convertir cada serie a dFC y guardar\n",
    "        for ser, is_aug in series_list:\n",
    "            dfc = fc_builder.compute_dynamic_fc(ser)  # (W_i, 116, 116)\n",
    "            if not np.isfinite(dfc).all():\n",
    "                # si algo rompiÃ³ la numerica, salta esta entrada\n",
    "                continue\n",
    "\n",
    "            out_obj[\"data\"].append(dfc)\n",
    "            out_obj[\"labels\"].append(int(y))\n",
    "            out_obj[\"augmented\"].append(bool(is_aug))\n",
    "            out_obj[\"source_index\"].append(int(subj_idx))\n",
    "            mats_por_sample.append(dfc.shape[0])\n",
    "\n",
    "    out_path = os.path.join(OUT_DIR, f\"{site_name}_dfc_aug.pkl\")\n",
    "    joblib.dump(out_obj, out_path, compress=3)\n",
    "\n",
    "    print(f\"[{site_name}] sujetos originales: {len(ts_list)} | muestras guardadas (orig+aug): {len(out_obj['data'])} | sujetos saltados: {skipped}\")\n",
    "    print(f\"â†’ {out_path}\")\n",
    "\n",
    "    return len(ts_list), len(out_obj[\"data\"]), mats_por_sample\n",
    "\n",
    "\n",
    "# ====== Ejecutar por sitio y resumen ======\n",
    "resumen_rows = []\n",
    "for site_name, site_data in all_data.items():\n",
    "    n_orig, n_muestras, mats_list = process_site_with_augmentation(site_name, site_data)\n",
    "\n",
    "    if len(mats_list) == 0:\n",
    "        prom_w = 0\n",
    "        total_w = 0\n",
    "    else:\n",
    "        prom_w = int(np.round(np.mean(mats_list)))  # ventanas promedio por muestra (orig+aug)\n",
    "        total_w = int(np.sum(mats_list))            # total de matrices dFC (todas las muestras)\n",
    "\n",
    "    resumen_rows.append({\n",
    "        \"Sitio\": site_name,\n",
    "        \"Sujetos originales\": n_orig,\n",
    "        \"Muestras (orig+aug)\": n_muestras,\n",
    "        \"Matrices FC por muestra\": prom_w,\n",
    "        \"Matrices FC por sitio\": total_w,\n",
    "    })\n",
    "\n",
    "df_resumen = pd.DataFrame(resumen_rows)\n",
    "print(\"\\nResumen por sitio (con aumentaciÃ³n):\")\n",
    "print(df_resumen)\n",
    "\n",
    "# (opcional) CSV de resumen\n",
    "df_resumen.to_csv(os.path.join(OUT_DIR, \"resumen_dfc_aug_por_sitio.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3b78b",
   "metadata": {},
   "source": [
    "## __Channel Builder__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b23e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import expm, eigh\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from scipy.stats import entropy\n",
    "import networkx as nx\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class FcChannelBuilder:\n",
    "    def __init__(self,\n",
    "                 use_mean=True,\n",
    "                 use_abs=True,\n",
    "                 use_std=True,\n",
    "                 use_max=True,\n",
    "                 use_min=True,\n",
    "                 use_diffusion=False,\n",
    "                 use_spec=False,\n",
    "                 use_clustering=False,\n",
    "                 use_entropy=False,\n",
    "                 use_ddt=False,\n",
    "                 eig_k=4,\n",
    "                 ddt_scale=1.0,\n",
    "                 t_list=None,\n",
    "                 n_jobs=0,\n",
    "                 verbose=False):\n",
    "        # Canales base\n",
    "        self.use_mean = use_mean\n",
    "        self.use_abs = use_abs\n",
    "        self.use_std = use_std\n",
    "        self.use_max = use_max\n",
    "        self.use_min = use_min\n",
    "\n",
    "        # Canales agregados\n",
    "        self.use_diffusion = use_diffusion\n",
    "        self.use_spec = use_spec\n",
    "        self.use_clustering = use_clustering\n",
    "        self.use_entropy = use_entropy\n",
    "        self.use_ddt = use_ddt\n",
    "\n",
    "        # ParÃ¡metros adicionales\n",
    "        self.eig_k = eig_k\n",
    "        self.ddt_scale = ddt_scale\n",
    "        self.t_list = [1.0] if t_list is None else t_list\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def build(self, fcs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        fcs: np.ndarray of shape (W, N, N), Fisher-z transformed\n",
    "        Returns: np.ndarray of shape (C, N, N)\n",
    "        \"\"\"\n",
    "        if not isinstance(fcs, np.ndarray) or fcs.ndim != 3:\n",
    "            raise ValueError(\"âŒ fcs debe ser un arreglo NumPy de shape (W, N, N)\")\n",
    "\n",
    "        chans = []\n",
    "\n",
    "        if self.use_mean:\n",
    "            chan = fcs.mean(axis=0)\n",
    "            chans.append(chan)\n",
    "            if self.verbose: print(\"[mean] shape:\", chan.shape)\n",
    "\n",
    "        if self.use_abs:\n",
    "            chan = np.mean(np.abs(fcs), axis=0)\n",
    "            chans.append(chan)\n",
    "            if self.verbose: print(\"[abs] shape:\", chan.shape)\n",
    "\n",
    "        if self.use_std:\n",
    "            chan = fcs.std(axis=0)\n",
    "            chans.append(chan)\n",
    "            if self.verbose: print(\"[std] shape:\", chan.shape)\n",
    "\n",
    "        if self.use_max:\n",
    "            chan = fcs.max(axis=0)\n",
    "            chans.append(chan)\n",
    "            if self.verbose: print(\"[max] shape:\", chan.shape)\n",
    "\n",
    "        if self.use_min:\n",
    "            chan = fcs.min(axis=0)\n",
    "            chans.append(chan)\n",
    "            if self.verbose: print(\"[min] shape:\", chan.shape)\n",
    "\n",
    "        if self.use_diffusion:\n",
    "            chans.append(self._diffusion_channels(fcs))\n",
    "\n",
    "        if self.use_spec:\n",
    "            chans.append(self._laplacian_eig_channel(fcs, k=self.eig_k))\n",
    "\n",
    "        if self.use_clustering:\n",
    "            chans.append(self._clustering_channel(fcs))\n",
    "\n",
    "        if self.use_entropy:\n",
    "            chans.append(self._entropy_channel(fcs))\n",
    "\n",
    "        if self.use_ddt:\n",
    "            chans.append(self._ddt_channel(fcs))\n",
    "\n",
    "        return np.stack(chans, axis=0).astype(np.float32, copy=False)\n",
    "\n",
    "    # -------------------------------\n",
    "    # MÃ©todos auxiliares\n",
    "    # -------------------------------\n",
    "\n",
    "    def _diffusion_single(self, z, t):\n",
    "        adj = np.abs(z)\n",
    "        L = laplacian(adj, normed=True)\n",
    "        return expm(-t * L)\n",
    "\n",
    "    def _diffusion_channels(self, fcs):\n",
    "        results = []\n",
    "        for t in self.t_list:\n",
    "            diffs = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(self._diffusion_single)(z, t) for z in fcs\n",
    "            )\n",
    "            chan_t = np.mean(diffs, axis=0, keepdims=True)\n",
    "            results.append(chan_t)\n",
    "        return np.concatenate(results, axis=0)  # shape: (len(t_list), N, N)\n",
    "\n",
    "    def _laplacian_eig_channel(self, fcs, k=4):\n",
    "        def eig_spec(z):\n",
    "            adj = np.abs(z)\n",
    "            L = laplacian(adj, normed=True)\n",
    "            _, eigvecs = eigh(L)\n",
    "            topk = eigvecs[:, :k]\n",
    "            return topk @ topk.T  # (N, N)\n",
    "\n",
    "        ev_matrices = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(eig_spec)(z) for z in fcs\n",
    "        )\n",
    "        return np.mean(ev_matrices, axis=0, keepdims=True)  # shape: (1, N, N)\n",
    "\n",
    "    def _clustering_channel(self, fcs):\n",
    "        def cluster_diag(z):\n",
    "            G = nx.from_numpy_array(np.abs(z))\n",
    "            clust = nx.clustering(G)\n",
    "            vec = np.array([clust[i] for i in range(z.shape[0])])\n",
    "            return np.diag(vec)\n",
    "\n",
    "        mats = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(cluster_diag)(z) for z in fcs\n",
    "        )\n",
    "        return np.mean(mats, axis=0, keepdims=True)\n",
    "\n",
    "    def _entropy_channel(self, fcs):\n",
    "        def entropy_diag(z):\n",
    "            abs_z = np.abs(z)\n",
    "            norm_z = abs_z / (abs_z.sum(axis=1, keepdims=True) + 1e-6)\n",
    "            ent = entropy(norm_z.T)\n",
    "            return np.diag(ent)\n",
    "\n",
    "        mats = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(entropy_diag)(z) for z in fcs\n",
    "        )\n",
    "        return np.mean(mats, axis=0, keepdims=True)\n",
    "\n",
    "    def _ddt_channel(self, fcs):\n",
    "        def ddt_transform(z):\n",
    "            adj = np.abs(z)\n",
    "            L = laplacian(adj, normed=True)\n",
    "            return expm(-self.ddt_scale * L)\n",
    "\n",
    "        mats = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(ddt_transform)(z) for z in fcs\n",
    "        )\n",
    "        return np.mean(mats, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aecaf60",
   "metadata": {},
   "source": [
    "## __Example__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0fa8e66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BoldAugmentor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Instancias\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m augmentor = \u001b[43mBoldAugmentor\u001b[49m()\n\u001b[32m      6\u001b[39m fc_builder = DynamicFcBuilder(win_len=\u001b[32m40\u001b[39m, step=\u001b[32m15\u001b[39m)\n\u001b[32m      7\u001b[39m channel_builder = FcChannelBuilder(\n\u001b[32m      8\u001b[39m     use_mean=\u001b[38;5;28;01mTrue\u001b[39;00m,         \u001b[38;5;66;03m# Debe estar en True\u001b[39;00m\n\u001b[32m      9\u001b[39m     use_abs=\u001b[38;5;28;01mTrue\u001b[39;00m,          \u001b[38;5;66;03m# TambiÃ©n en True\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     use_min=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'BoldAugmentor' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Instancias\n",
    "augmentor = BoldAugmentor()\n",
    "fc_builder = DynamicFcBuilder(win_len=40, step=15)\n",
    "channel_builder = FcChannelBuilder(\n",
    "    use_mean=True,         # Debe estar en True\n",
    "    use_abs=True,          # TambiÃ©n en True\n",
    "    use_std=True,\n",
    "    use_max=True,\n",
    "    use_min=True\n",
    ")\n",
    "\n",
    "# 2. Almacenar resultados\n",
    "site_channel_maps = {}\n",
    "\n",
    "for site_name, site_data in tqdm(all_data.items(), desc=\"Procesando sitios\"):\n",
    "    site_channels = []\n",
    "\n",
    "    for ts in site_data[\"data\"]:\n",
    "        # A. AugmentaciÃ³n\n",
    "        ts_aug = augmentor.augment(ts.copy())\n",
    "\n",
    "        # B. FC dinÃ¡mica (W, N, N)\n",
    "        fcs = fc_builder.compute_dynamic_fc(ts_aug)\n",
    "\n",
    "        # C. Canales (C, N, N)\n",
    "        chans = channel_builder.build(fcs)\n",
    "\n",
    "        site_channels.append(chans)\n",
    "        \n",
    "    site_channel_maps[site_name] = site_channels\n",
    "\n",
    "    print(f\"ðŸ“ Sitio: {site_name}\")\n",
    "    print(f\"   â†’ Sujetos procesados: {len(site_channels)}\")\n",
    "    if len(site_channels) > 0:\n",
    "        print(f\"   â†’ Forma de los canales: {site_channels[0].shape}\")\n",
    "        print(f\"   â†’ Tipo: {type(site_channels[0])}\")\n",
    "        print(f\"   â†’ Rango de valores: min={site_channels[0].min():.3f}, max={site_channels[0].max():.3f}, mean={site_channels[0].mean():.3f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
